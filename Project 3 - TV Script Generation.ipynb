{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Parse-the-data\" data-toc-modified-id=\"Parse-the-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Parse the data</a></span></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Exploration</a></span></li><li><span><a href=\"#Pre-processing-Functions\" data-toc-modified-id=\"Pre-processing-Functions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Pre-processing Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Look-up-Table\" data-toc-modified-id=\"Look-up-Table-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Look-up Table</a></span></li><li><span><a href=\"#Tokenize-Punctuation\" data-toc-modified-id=\"Tokenize-Punctuation-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Tokenize Punctuation</a></span></li><li><span><a href=\"#Apply-the-pre-processing-functions\" data-toc-modified-id=\"Apply-the-pre-processing-functions-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Apply the pre-processing functions</a></span></li></ul></li><li><span><a href=\"#Neural-Network\" data-toc-modified-id=\"Neural-Network-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Neural Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input:-Batching\" data-toc-modified-id=\"Input:-Batching-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Input: Batching</a></span></li><li><span><a href=\"#Neural-Network-Architecture\" data-toc-modified-id=\"Neural-Network-Architecture-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Neural Network Architecture</a></span></li><li><span><a href=\"#Forward-and-Backpropagation\" data-toc-modified-id=\"Forward-and-Backpropagation-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Forward and Backpropagation</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-Function\" data-toc-modified-id=\"Training-Function-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Training Function</a></span></li><li><span><a href=\"#Hyperparameters\" data-toc-modified-id=\"Hyperparameters-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Hyperparameters</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Train</a></span></li></ul></li><li><span><a href=\"#Generate-a-TV-Script\" data-toc-modified-id=\"Generate-a-TV-Script-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Generate a TV Script</a></span><ul class=\"toc-item\"><li><span><a href=\"#function-to-generate-text\" data-toc-modified-id=\"function-to-generate-text-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>function to generate text</a></span></li><li><span><a href=\"#Apply-the-function-to-make-a-script\" data-toc-modified-id=\"Apply-the-function-to-make-a-script-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Apply the function to make a script</a></span></li><li><span><a href=\"#Save-a-Script\" data-toc-modified-id=\"Save-a-Script-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Save a Script</a></span></li></ul></li><li><span><a href=\"#Submitting-This-Project\" data-toc-modified-id=\"Submitting-This-Project-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Submitting This Project</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/seinfeld-chronicles/scripts.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Character</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>EpisodeNo</th>\n",
       "      <th>SEID</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>JERRY</td>\n",
       "      <td>Do you know what this is all about? Do you kno...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>JERRY</td>\n",
       "      <td>(pointing at Georges shirt) See, to me, that b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Are you through?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>JERRY</td>\n",
       "      <td>You do of course try on, when you buy?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Yes, it was purple, I liked it, I dont actuall...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Character                                           Dialogue  \\\n",
       "0           0     JERRY  Do you know what this is all about? Do you kno...   \n",
       "1           1     JERRY  (pointing at Georges shirt) See, to me, that b...   \n",
       "2           2    GEORGE                                   Are you through?   \n",
       "3           3     JERRY             You do of course try on, when you buy?   \n",
       "4           4    GEORGE  Yes, it was purple, I liked it, I dont actuall...   \n",
       "\n",
       "   EpisodeNo    SEID  Season  \n",
       "0        1.0  S01E01     1.0  \n",
       "1        1.0  S01E01     1.0  \n",
       "2        1.0  S01E01     1.0  \n",
       "3        1.0  S01E01     1.0  \n",
       "4        1.0  S01E01     1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts_df = pd.read_csv(data_dir)\n",
    "scripts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the data\n",
    "Desired format:\n",
    "<br>\n",
    "all one string, with the character name, colon, then lower-cased dialoge. \n",
    "<br>\n",
    "Example\n",
    "<br>\n",
    "'jerry:  do you know what this is all about? do you know, why were here?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format text\n",
    "def scriptParser(character, dialogue):\n",
    "    if(isinstance(dialogue, str)):\n",
    "        dialogue = dialogue.lower()\n",
    "    else:\n",
    "        #dialogue = str(dialogue)\n",
    "        #print(dialogue) - always a nan\n",
    "        dialogue = ''\n",
    "    if(isinstance(character, str)):\n",
    "        character = character.lower()\n",
    "    else:\n",
    "        #character = str(character)\n",
    "        character = ''\n",
    "    \n",
    "    return character+': '+dialogue+'\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people tryin to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, whatta you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go.\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function on one row\n",
    "row = 0\n",
    "scriptParser(scripts_df['Character'].iloc[row], scripts_df['Dialogue'].iloc[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people tryin to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, whatta you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go.\\n\\njerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother.\\n\\ngeorge: are you through?\\n\\njerry: you do of course try on, when you buy?\\n\\ngeorge: yes, it was purple, i liked it, i dont actually recall considering the buttons.\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function while joining on 5 rows\n",
    "''.join([scriptParser(row['Character'], row['Dialogue']) \n",
    "         for index, row in scripts_df[:5].iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function\n",
    "text = ''.join([scriptParser(row['Character'], row['Dialogue']) \n",
    "         for index, row in scripts_df.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "~ Number of unique words: 46380\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544121282030155\n",
      "\n",
      "The lines 0 to 10:\n",
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people tryin to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, whatta you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go.\n",
      "\n",
      "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother.\n",
      "\n",
      "george: are you through?\n",
      "\n",
      "jerry: you do of course try on, when you buy?\n",
      "\n",
      "george: yes, it was purple, i liked it, i dont actually recall considering the buttons.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_line_range = (0, 10)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('~ Number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look-up Table\n",
    "2 dictionaries:\n",
    "<br>\n",
    "1. word to index: words2idx \n",
    "2. index to word: idx2word \n",
    "\n",
    "More common words should have a lower index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    \n",
    "    text: The text of tv scripts split into words\n",
    "    returns: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # first get the word_counts\n",
    "    word_counts = Counter(text)\n",
    "    \n",
    "    # sort from most to least frequent\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    # define the dictionaries\n",
    "    idx2word = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    word2idx = {word: ii for ii, word in idx2word.items()}\n",
    "    \n",
    "    # return tuple\n",
    "    return (idx2word, word2idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict to turn punctuation into a token.\n",
    "punct2token = {'.': '<PERIOD>',\n",
    "                ',': '<COMMA>',\n",
    "                '\"': '<QUOTATION_MARK>',\n",
    "                ';': '<SEMICOLON>',\n",
    "                '!': '<EXCLAMATION_MARK>',\n",
    "                '?': '<QUESTION_MARK>',\n",
    "                '(': '<LEFT_PAREN>',\n",
    "                ')': '<RIGHT_PAREN>',\n",
    "                '--': ' <HYPHENS> ',\n",
    "                '-': '<DASH>',\n",
    "                '?': '<QUESTION_MARK>',\n",
    "                '\\n': '<NEW_LINE>',\n",
    "                ':': ' <COLON> '}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<COMMA>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct2token[',']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the original text \n",
    "raw_text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the punctuation\n",
    "for punct, token in punct2token.items():\n",
    "    text = text.replace(punct, ' {} '.format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and make all ensure all text is lower case\n",
    "text = text.lower()\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vocab dictionaries\n",
    "idx2word, word2idx = create_lookup_tables(text + ['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply dictionaries to text\n",
    "int_text = [word2idx[word] for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 2,\n",
       " 35,\n",
       " 5,\n",
       " 27,\n",
       " 19,\n",
       " 24,\n",
       " 23,\n",
       " 49,\n",
       " 58,\n",
       " 4,\n",
       " 35,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 80,\n",
       " 119,\n",
       " 61,\n",
       " 4,\n",
       " 9,\n",
       " 54,\n",
       " 47,\n",
       " 3,\n",
       " 24,\n",
       " 23,\n",
       " 47,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 18,\n",
       " 47,\n",
       " 23,\n",
       " 79,\n",
       " 21,\n",
       " 7,\n",
       " 1279,\n",
       " 547,\n",
       " 8340,\n",
       " 6825,\n",
       " 21,\n",
       " 240,\n",
       " 1,\n",
       " 147,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 81,\n",
       " 5,\n",
       " 198,\n",
       " 235,\n",
       " 147,\n",
       " 206,\n",
       " 58,\n",
       " 55,\n",
       " 133,\n",
       " 63,\n",
       " 47,\n",
       " 4,\n",
       " 24,\n",
       " 23,\n",
       " 19,\n",
       " 692,\n",
       " 206,\n",
       " 58,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 24,\n",
       " 218,\n",
       " 125,\n",
       " 3,\n",
       " 119,\n",
       " 49,\n",
       " 47,\n",
       " 83,\n",
       " 3,\n",
       " 26,\n",
       " 79,\n",
       " 23,\n",
       " 288,\n",
       " 1,\n",
       " 45,\n",
       " 79,\n",
       " 375,\n",
       " 61,\n",
       " 23,\n",
       " 288,\n",
       " 3,\n",
       " 119,\n",
       " 49,\n",
       " 47,\n",
       " 12,\n",
       " 75,\n",
       " 48,\n",
       " 147,\n",
       " 6826,\n",
       " 9,\n",
       " 247,\n",
       " 191,\n",
       " 3,\n",
       " 64,\n",
       " 202,\n",
       " 27,\n",
       " 127,\n",
       " 55,\n",
       " 48,\n",
       " 1,\n",
       " 13,\n",
       " 29,\n",
       " 95,\n",
       " 2681,\n",
       " 171,\n",
       " 14,\n",
       " 81,\n",
       " 5,\n",
       " 1403,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 388,\n",
       " 247,\n",
       " 68,\n",
       " 1,\n",
       " 127,\n",
       " 81,\n",
       " 36,\n",
       " 63,\n",
       " 4,\n",
       " 36,\n",
       " 530,\n",
       " 94,\n",
       " 25,\n",
       " 127,\n",
       " 36,\n",
       " 42,\n",
       " 78,\n",
       " 1,\n",
       " 36,\n",
       " 337,\n",
       " 40,\n",
       " 577,\n",
       " 47,\n",
       " 1,\n",
       " 5,\n",
       " 213,\n",
       " 63,\n",
       " 47,\n",
       " 5,\n",
       " 51,\n",
       " 426,\n",
       " 3,\n",
       " 5,\n",
       " 323,\n",
       " 47,\n",
       " 7,\n",
       " 561,\n",
       " 3,\n",
       " 57,\n",
       " 4,\n",
       " 5,\n",
       " 100,\n",
       " 7,\n",
       " 732,\n",
       " 3,\n",
       " 5,\n",
       " 51,\n",
       " 49,\n",
       " 426,\n",
       " 3,\n",
       " 51,\n",
       " 7,\n",
       " 1253,\n",
       " 3,\n",
       " 51,\n",
       " 52,\n",
       " 353,\n",
       " 3,\n",
       " 7,\n",
       " 158,\n",
       " 3,\n",
       " 7,\n",
       " 834,\n",
       " 3,\n",
       " 7,\n",
       " 2954,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 114,\n",
       " 313,\n",
       " 674,\n",
       " 173,\n",
       " 3,\n",
       " 4640,\n",
       " 5,\n",
       " 35,\n",
       " 4,\n",
       " 5,\n",
       " 63,\n",
       " 55,\n",
       " 138,\n",
       " 54,\n",
       " 188,\n",
       " 87,\n",
       " 1,\n",
       " 413,\n",
       " 313,\n",
       " 47,\n",
       " 3,\n",
       " 5,\n",
       " 213,\n",
       " 51,\n",
       " 87,\n",
       " 12,\n",
       " 5,\n",
       " 213,\n",
       " 63,\n",
       " 9,\n",
       " 509,\n",
       " 3,\n",
       " 5,\n",
       " 213,\n",
       " 51,\n",
       " 50,\n",
       " 3,\n",
       " 5,\n",
       " 213,\n",
       " 63,\n",
       " 47,\n",
       " 178,\n",
       " 382,\n",
       " 3,\n",
       " 57,\n",
       " 4,\n",
       " 127,\n",
       " 198,\n",
       " 5,\n",
       " 48,\n",
       " 22,\n",
       " 240,\n",
       " 3,\n",
       " 193,\n",
       " 33,\n",
       " 726,\n",
       " 3,\n",
       " 1567,\n",
       " 138,\n",
       " 63,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 13,\n",
       " 526,\n",
       " 59,\n",
       " 4641,\n",
       " 417,\n",
       " 14,\n",
       " 74,\n",
       " 3,\n",
       " 9,\n",
       " 25,\n",
       " 3,\n",
       " 20,\n",
       " 1041,\n",
       " 23,\n",
       " 22,\n",
       " 7,\n",
       " 945,\n",
       " 922,\n",
       " 834,\n",
       " 1,\n",
       " 7,\n",
       " 263,\n",
       " 1041,\n",
       " 3342,\n",
       " 376,\n",
       " 170,\n",
       " 1984,\n",
       " 7,\n",
       " 417,\n",
       " 3,\n",
       " 88,\n",
       " 59,\n",
       " 17,\n",
       " 1,\n",
       " 193,\n",
       " 132,\n",
       " 520,\n",
       " 12,\n",
       " 193,\n",
       " 22,\n",
       " 26,\n",
       " 34,\n",
       " 4217,\n",
       " 34,\n",
       " 1985,\n",
       " 1,\n",
       " 5,\n",
       " 88,\n",
       " 46,\n",
       " 5,\n",
       " 410,\n",
       " 41,\n",
       " 52,\n",
       " 411,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 48,\n",
       " 5,\n",
       " 275,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 35,\n",
       " 21,\n",
       " 260,\n",
       " 380,\n",
       " 29,\n",
       " 3,\n",
       " 117,\n",
       " 5,\n",
       " 441,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 97,\n",
       " 3,\n",
       " 17,\n",
       " 42,\n",
       " 5163,\n",
       " 3,\n",
       " 6,\n",
       " 747,\n",
       " 17,\n",
       " 3,\n",
       " 6,\n",
       " 202,\n",
       " 315,\n",
       " 3139,\n",
       " 2834,\n",
       " 7,\n",
       " 2367,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 28,\n",
       " 3,\n",
       " 5,\n",
       " 202,\n",
       " 3139,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 13,\n",
       " 29,\n",
       " 95,\n",
       " 2681,\n",
       " 3140,\n",
       " 14,\n",
       " 71,\n",
       " 3,\n",
       " 26,\n",
       " 3,\n",
       " 45,\n",
       " 59,\n",
       " 24,\n",
       " 112,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 32,\n",
       " 3,\n",
       " 11392,\n",
       " 3,\n",
       " 1229,\n",
       " 44,\n",
       " 46,\n",
       " 9,\n",
       " 27,\n",
       " 3,\n",
       " 19,\n",
       " 5,\n",
       " 453,\n",
       " 18,\n",
       " 117,\n",
       " 5,\n",
       " 453,\n",
       " 17,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2955,\n",
       " 2,\n",
       " 121,\n",
       " 1,\n",
       " 359,\n",
       " 1,\n",
       " 121,\n",
       " 1,\n",
       " 356,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 48,\n",
       " 3,\n",
       " 48,\n",
       " 5,\n",
       " 161,\n",
       " 24,\n",
       " 23,\n",
       " 3563,\n",
       " 4,\n",
       " 2682,\n",
       " 7,\n",
       " 2278,\n",
       " 11393,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2955,\n",
       " 2,\n",
       " 193,\n",
       " 1280,\n",
       " 3,\n",
       " 6,\n",
       " 40,\n",
       " 9,\n",
       " 35,\n",
       " 17,\n",
       " 22,\n",
       " 33,\n",
       " 228,\n",
       " 3563,\n",
       " 277,\n",
       " 3,\n",
       " 1740,\n",
       " 57,\n",
       " 3,\n",
       " 3563,\n",
       " 277,\n",
       " 3,\n",
       " 1740,\n",
       " 57,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 193,\n",
       " 103,\n",
       " 5164,\n",
       " 208,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 77,\n",
       " 5,\n",
       " 1438,\n",
       " 3,\n",
       " 193,\n",
       " 10,\n",
       " 959,\n",
       " 21,\n",
       " 231,\n",
       " 1,\n",
       " 2955,\n",
       " 23,\n",
       " 10,\n",
       " 3564,\n",
       " 500,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2955,\n",
       " 2,\n",
       " 1343,\n",
       " 25,\n",
       " 11,\n",
       " 1,\n",
       " 26,\n",
       " 79,\n",
       " 181,\n",
       " 160,\n",
       " 1854,\n",
       " 22,\n",
       " 556,\n",
       " 5,\n",
       " 29,\n",
       " 4218,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 72,\n",
       " 90,\n",
       " 313,\n",
       " 45,\n",
       " 2211,\n",
       " 7,\n",
       " 263,\n",
       " 209,\n",
       " 382,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 32,\n",
       " 3,\n",
       " 712,\n",
       " 24,\n",
       " 71,\n",
       " 3,\n",
       " 136,\n",
       " 350,\n",
       " 54,\n",
       " 4642,\n",
       " 22,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 164,\n",
       " 10,\n",
       " 263,\n",
       " 3,\n",
       " 164,\n",
       " 10,\n",
       " 263,\n",
       " 3,\n",
       " 19,\n",
       " 284,\n",
       " 22,\n",
       " 3,\n",
       " 19,\n",
       " 136,\n",
       " 23,\n",
       " 284,\n",
       " 22,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 183,\n",
       " 5,\n",
       " 58,\n",
       " 1115,\n",
       " 3,\n",
       " 7,\n",
       " 548,\n",
       " 6,\n",
       " 476,\n",
       " 22,\n",
       " 3565,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 26,\n",
       " 3,\n",
       " 5,\n",
       " 530,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 194,\n",
       " 6,\n",
       " 183,\n",
       " 5,\n",
       " 58,\n",
       " 17,\n",
       " 3,\n",
       " 97,\n",
       " 3,\n",
       " 67,\n",
       " 11394,\n",
       " 8341,\n",
       " 2683,\n",
       " 4,\n",
       " 6,\n",
       " 476,\n",
       " 62,\n",
       " 7,\n",
       " 207,\n",
       " 6,\n",
       " 81,\n",
       " 7,\n",
       " 209,\n",
       " 22,\n",
       " 11395,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 195,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 13,\n",
       " 214,\n",
       " 22,\n",
       " 7,\n",
       " 11396,\n",
       " 14,\n",
       " 712,\n",
       " 26,\n",
       " 1377,\n",
       " 22,\n",
       " 61,\n",
       " 3,\n",
       " 19,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 3,\n",
       " 19,\n",
       " 23,\n",
       " 67,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 13,\n",
       " 347,\n",
       " 7,\n",
       " 1377,\n",
       " 77,\n",
       " 92,\n",
       " 8,\n",
       " 18,\n",
       " 569,\n",
       " 17,\n",
       " 29,\n",
       " 7,\n",
       " 300,\n",
       " 14,\n",
       " 19,\n",
       " 23,\n",
       " 67,\n",
       " 46,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 28,\n",
       " 3,\n",
       " 693,\n",
       " 82,\n",
       " 174,\n",
       " 1,\n",
       " 6,\n",
       " 116,\n",
       " 3,\n",
       " 693,\n",
       " 60,\n",
       " 46,\n",
       " 10,\n",
       " 338,\n",
       " 8342,\n",
       " 58,\n",
       " 62,\n",
       " 18,\n",
       " 693,\n",
       " 82,\n",
       " 2956,\n",
       " 18,\n",
       " 82,\n",
       " 309,\n",
       " 18,\n",
       " 71,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 814,\n",
       " 793,\n",
       " 3,\n",
       " 6,\n",
       " 116,\n",
       " 3,\n",
       " 17,\n",
       " 42,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 206,\n",
       " 41,\n",
       " 62,\n",
       " 23,\n",
       " 46,\n",
       " 206,\n",
       " 41,\n",
       " 5,\n",
       " 3,\n",
       " 73,\n",
       " 3,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 780,\n",
       " 175,\n",
       " 219,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 13,\n",
       " 684,\n",
       " 14,\n",
       " 38,\n",
       " 3,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 19,\n",
       " 3,\n",
       " 19,\n",
       " 204,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 28,\n",
       " 3,\n",
       " 184,\n",
       " 204,\n",
       " 3,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 73,\n",
       " 23,\n",
       " 42,\n",
       " 174,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 28,\n",
       " 3,\n",
       " 184,\n",
       " 204,\n",
       " 3,\n",
       " 73,\n",
       " 17,\n",
       " 42,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 30,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 24,\n",
       " 23,\n",
       " 174,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 30,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 38,\n",
       " 3,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 67,\n",
       " 835,\n",
       " 18,\n",
       " 357,\n",
       " 67,\n",
       " 369,\n",
       " 9,\n",
       " 63,\n",
       " 47,\n",
       " 41,\n",
       " 5,\n",
       " 382,\n",
       " 207,\n",
       " 4,\n",
       " 217,\n",
       " 1916,\n",
       " 12,\n",
       " 3859,\n",
       " 5,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 30,\n",
       " 3,\n",
       " 32,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 45,\n",
       " 611,\n",
       " 1,\n",
       " 6,\n",
       " 116,\n",
       " 3,\n",
       " 67,\n",
       " 143,\n",
       " 3,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 67,\n",
       " 341,\n",
       " 24,\n",
       " 427,\n",
       " 18,\n",
       " 143,\n",
       " 67,\n",
       " 102,\n",
       " 9,\n",
       " 90,\n",
       " 22,\n",
       " 39,\n",
       " 10,\n",
       " 6827,\n",
       " 18,\n",
       " 146,\n",
       " 32,\n",
       " 51,\n",
       " 318,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 13,\n",
       " 4643,\n",
       " 6828,\n",
       " 14,\n",
       " 585,\n",
       " 585,\n",
       " 585,\n",
       " 3,\n",
       " 102,\n",
       " 9,\n",
       " 4,\n",
       " 102,\n",
       " 9,\n",
       " 90,\n",
       " 22,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 30,\n",
       " 3,\n",
       " 73,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 102,\n",
       " 9,\n",
       " 90,\n",
       " 22,\n",
       " 18,\n",
       " 146,\n",
       " 32,\n",
       " 51,\n",
       " 318,\n",
       " 4,\n",
       " 102,\n",
       " 9,\n",
       " 18,\n",
       " 146,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 30,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 26,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 26,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 26,\n",
       " 3,\n",
       " 6,\n",
       " 482,\n",
       " 9,\n",
       " 94,\n",
       " 5,\n",
       " 24,\n",
       " 1,\n",
       " 313,\n",
       " 45,\n",
       " 85,\n",
       " 74,\n",
       " 24,\n",
       " 136,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 19,\n",
       " 3,\n",
       " 48,\n",
       " 5,\n",
       " 894,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 80,\n",
       " 3,\n",
       " 80,\n",
       " 81,\n",
       " 67,\n",
       " 152,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 2,\n",
       " 72,\n",
       " 35,\n",
       " 6,\n",
       " 27,\n",
       " 3,\n",
       " 146,\n",
       " 3,\n",
       " 5,\n",
       " 27,\n",
       " 3,\n",
       " 146,\n",
       " 67,\n",
       " 312,\n",
       " ...]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a pickle file\n",
    "pickle.dump((int_text, idx2word, word2idx, punct2token), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "# Check GPU Access\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input: Batching\n",
    "We want to batch the data based on a provided sequence legnth\n",
    "Example Input:\n",
    "```\n",
    "words = [1, 2, 3, 4, 5, 6, 7]\n",
    "sequence_length = 4\n",
    "```\n",
    "\n",
    "`features` Tensor:\n",
    "```\n",
    "[1, 2, 3, 4]\n",
    "```\n",
    "`target` Tensor (the next \"word\"):\n",
    "```\n",
    "5\n",
    "```\n",
    "\n",
    "Next `features` and `target` Tensors:\n",
    "```\n",
    "[2, 3, 4, 5]  # features\n",
    "6             # target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    param words: The word ids of the TV scripts\n",
    "    param sequence_length: The sequence length of each batch\n",
    "    param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    \n",
    "    # iterate overall all words\n",
    "    features = []\n",
    "    targets = []\n",
    "    for i, w in enumerate(words):\n",
    "        if((i+sequence_length) < len(words)):\n",
    "            features.append(words[i:(i+sequence_length)])\n",
    "            targets.append(words[i+sequence_length])\n",
    "    \n",
    "    features = np.array(features)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # convert numpy arrays into a torch tensor dataset\n",
    "    sequenced_data = TensorDataset(torch.from_numpy(features), torch.from_numpy(targets))\n",
    "    \n",
    "    # create the final batch DataLoader\n",
    "    batch_dL = DataLoader(sequenced_data, shuffle=True, batch_size=batch_size) \n",
    "\n",
    "    # return a dataloader\n",
    "    return batch_dL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[42, 43, 44, 45, 46],\n",
      "        [ 8,  9, 10, 11, 12],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [28, 29, 30, 31, 32],\n",
      "        [41, 42, 43, 44, 45],\n",
      "        [24, 25, 26, 27, 28],\n",
      "        [ 9, 10, 11, 12, 13],\n",
      "        [44, 45, 46, 47, 48],\n",
      "        [35, 36, 37, 38, 39],\n",
      "        [20, 21, 22, 23, 24]], dtype=torch.int32)\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([47, 13, 15, 33, 46, 29, 14, 49, 40, 25], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Test the Data Loader\n",
    "\n",
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "Define an LSTM-based RNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "       \n",
    "         # set class variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = dropout\n",
    "        self.batch_size = None\n",
    "        \n",
    "       \n",
    "        # define model layers\n",
    "        \n",
    "        # embedding layer \n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_size)\n",
    "        \n",
    "        # initialize some weights in the network \n",
    "        self.init_weights()\n",
    "       \n",
    "    \n",
    "    def init_weights(self):\n",
    "        i_range = 0.08\n",
    "        \n",
    "        self.embed.weight.data.uniform_(-i_range, i_range)\n",
    "        \n",
    "        self.lstm.weight_ih_l0.data.uniform_(-i_range, i_range)\n",
    "        self.lstm.weight_hh_l0.data.uniform_(-i_range, i_range)\n",
    "        \n",
    "        self.lstm.bias_ih_l0.data.zero_()\n",
    "        self.lstm.bias_hh_l0.data.zero_()\n",
    "        \n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.normal_(0.0, (1.0 /np.sqrt(self.fc.in_features)))\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # embeddings\n",
    "        x = self.embed(nn_input)\n",
    "        \n",
    "        # get the LSTM outputs\n",
    "        l_out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # stack up the LSTM outputs - aka shape the output\n",
    "        l_out = l_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout - doesn't seem to be helping\n",
    "        # out = self.dropout(l_out)\n",
    "        \n",
    "        # get the final output\n",
    "        out = self.fc(l_out)\n",
    "\n",
    "        # reshape into (batch_size, seq_length, output_size)\n",
    "        out = out.view(self.batch_size, -1, self.output_size)\n",
    "\n",
    "        # get last batch\n",
    "        out = out[:, -1]\n",
    "        \n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        # and move to GPU if available\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backpropagation\n",
    "Define a function \n",
    "```\n",
    "forward_back_prop()\n",
    "```\n",
    "That will be used in training to find the average loss over a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    param decoder: The PyTorch Module that holds the neural network\n",
    "    param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    param criterion: The PyTorch loss function\n",
    "    param inp: A batch of input to the neural network\n",
    "    param target: The target output for the batch of input\n",
    "    return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "    else:\n",
    "        inp, target = inp.long(), target.long()\n",
    "        \n",
    "    # create new varaibles for the hidden state\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    \n",
    "    # zero out any accumulated gradients\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    # get the model outputs\n",
    "    output, hidden = rnn(inp, hidden)\n",
    "    \n",
    "    # calculate the loss & perform backprop\n",
    "    loss = criterion(output.squeeze(), target)\n",
    "    loss.backward()\n",
    "    \n",
    "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "    clip = 5\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "    \n",
    "    # perform an optimization step\n",
    "    optimizer.step()\n",
    "\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the rnn\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "    \n",
    "    # initialize a high minimum loss\n",
    "    min_Loss = np.Inf\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                avg_Loss = np.average(batch_losses)\n",
    "                \n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, avg_Loss))\n",
    "                \n",
    "                if avg_Loss < min_Loss:\n",
    "                    min_Loss = avg_Loss\n",
    "                    helper.save_model('./save/trained_rnn_new', rnn)\n",
    "                    print('Model Trained and Saved')\n",
    "                    \n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in each sentence: 5.756923863539527\n"
     ]
    }
   ],
   "source": [
    "# Find the average sentence length to use for the sequence length\n",
    "temp = raw_text.replace('?', '.')\n",
    "temp = temp.replace('!', '.')\n",
    "sentences = temp.split('.')\n",
    "\n",
    "word_count_sentences = [len(sent.split()) for sent in sentences]\n",
    "print('Average number of words in each sentence: {}'.format(np.average(word_count_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "\n",
    "# Sequence Length\n",
    "sequence_length = 6  # of words in a sequence\n",
    "\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "\n",
    "# Number of Epochs\n",
    "num_epochs = 1 # go low on CPU - requires 10 to get below 3.5\n",
    "\n",
    "# Learning Rate\n",
    "learning_rate = 0.002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "\n",
    "# Embedding Dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1 epoch(s)...\n",
      "Epoch:    1/1     Loss: 5.48744264125824\n",
      "\n",
      "Epoch:    1/1     Loss: 5.0490784506797795\n",
      "\n",
      "Epoch:    1/1     Loss: 4.9951553716659545\n",
      "\n",
      "Epoch:    1/1     Loss: 4.996782721996308\n",
      "\n",
      "Epoch:    1/1     Loss: 4.972678839683533\n",
      "\n",
      "Epoch:    1/1     Loss: 4.9234851174354555\n",
      "\n",
      "Epoch:    1/1     Loss: 4.947990944385529\n",
      "\n",
      "Epoch:    1/1     Loss: 4.866194286346436\n",
      "\n",
      "Epoch:    1/1     Loss: 4.820928164482117\n",
      "\n",
      "Epoch:    1/1     Loss: 4.7875993790626525\n",
      "\n",
      "Epoch:    1/1     Loss: 4.828715405464172\n",
      "\n",
      "Epoch:    1/1     Loss: 4.804311979293823\n",
      "\n",
      "Epoch:    1/1     Loss: 4.818021938800812\n",
      "\n",
      "Epoch:    1/1     Loss: 4.731715711593628\n",
      "\n",
      "Epoch:    1/1     Loss: 4.747332892894745\n",
      "\n",
      "Epoch:    1/1     Loss: 4.727448909282685\n",
      "\n",
      "Epoch:    1/1     Loss: 4.698231386184692\n",
      "\n",
      "Epoch:    1/1     Loss: 4.773421789169311\n",
      "\n",
      "Epoch:    1/1     Loss: 4.697781580448151\n",
      "\n",
      "Epoch:    1/1     Loss: 4.716354287147522\n",
      "\n",
      "Epoch:    1/1     Loss: 4.679349272727967\n",
      "\n",
      "Epoch:    1/1     Loss: 4.741835752010346\n",
      "\n",
      "Epoch:    1/1     Loss: 4.7148343868255616\n",
      "\n",
      "Epoch:    1/1     Loss: 4.710278750419617\n",
      "\n",
      "Epoch:    1/1     Loss: 4.739653410434723\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-208-8b87078b8bb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# training the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrained_rnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_every_n_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# saving the trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-173-0bc3ee611061>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[1;34m(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;31m# forward, back prop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_back_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;31m# record loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mbatch_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-195-c939d9408447>\u001b[0m in \u001b[0;36mforward_back_prop\u001b[1;34m(rnn, optimizer, criterion, inp, target, hidden)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# calculate the loss & perform backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the trained model\n",
    "# save_filename = os.path.splitext(os.path.basename('./save/trained_rnn'))[0] + '.pt'\n",
    "# torch.save(trained_rnn, save_filename)\n",
    "# print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a TV Script\n",
    "To create text, provide the network with a single 'seed' word, and it will keep making predictions until it reaches a set length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a trained model\n",
    "filename = 'trained_rnn_new'\n",
    "save_filename = os.path.splitext(os.path.basename(filename))[0] + ' .pt'\n",
    "# trained_rnn = troch.load(save_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param decoder: The PyTorch Module that holds the trained neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the function to make a script\n",
    "start with a character name:\n",
    "- jerry\n",
    "- elaine\n",
    "- george\n",
    "- kramer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_length = 400 \n",
    "prime_word = 'jerry'\n",
    "\n",
    "\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "f =  open(\"generated_script_1.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save another copy as an HTML file by clicking \"File\" -> \"Download as..\"->\"html\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission. Once you download these files, compress them into one zip file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "181px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
